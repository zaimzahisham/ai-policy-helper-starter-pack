# Embedding Configuration
EMBEDDING_MODEL=local-384

# LLM Provider Configuration
# Options: stub (default, offline), openai, ollama
LLM_PROVIDER=stub

# OpenAI Configuration (required if LLM_PROVIDER=openai)
OPENAI_API_KEY=

# Ollama Configuration (if using Ollama)
OLLAMA_HOST=http://ollama:11434 # or http://host.docker.internal:11434 if on MAC
OLLAMA_MODEL=qwen2.5:0.5b

# Vector Store Configuration
# Options: qdrant (default), memory
VECTOR_STORE=qdrant

# Qdrant Collection Name
COLLECTION_NAME=policy_helper

# Chunking Configuration
CHUNK_SIZE=700
CHUNK_OVERLAP=80

# Data Directory (path to policy documents)
DATA_DIR=/app/data

# Frontend API Base URL
NEXT_PUBLIC_API_BASE=http://localhost:8000

# Logging Configuration
# Options: DEBUG, INFO (default), WARNING, ERROR
LOG_LEVEL=INFO
# Optional: Log file path (logs will also go to console). Leave empty for console-only logging.
# Example: /app/logs/app.log (will create logs directory automatically)
LOG_FILE_PATH=
